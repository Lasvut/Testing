#!/usr/bin/env python3
"""
Test Improved SVM Model Performance

Validates that the improved model achieves:
- <500 false positives (out of ~7000 normal samples)
- 85%+ accuracy
- 85%+ recall
- 85%+ precision

Tests multiple thresholds to find optimal balance.
"""

import os
import csv
import time
from improved_svm_detector import ImprovedSVMAnomalyDetector
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix


def load_csic_samples(csv_file, label, max_samples=10000):
    """Load samples from CSIC 2010 CSV dataset"""
    samples = []

    if not os.path.exists(csv_file):
        print(f"‚ùå Error: {csv_file} not found")
        return samples

    try:
        with open(csv_file, 'r', encoding='utf-8', errors='ignore') as f:
            reader = csv.reader(f)
            next(reader)  # Skip header

            for row in reader:
                if len(row) >= 3 and row[0].strip() == label:
                    url = row[-1].strip()
                    samples.append({
                        'ip': '192.168.1.100' if label == 'Normal' else '10.0.0.100',
                        'path': '',
                        'payload': url,
                        'timestamp': time.time()
                    })

                    if len(samples) >= max_samples:
                        break

    except Exception as e:
        print(f"‚ùå Error loading samples: {e}")

    return samples


def evaluate_threshold(detector, test_samples, y_test, threshold):
    """Evaluate model at specific threshold"""
    y_pred = []

    for sample in test_samples:
        is_attack, _, _ = detector.is_anomalous(sample, threshold=threshold)
        y_pred.append(1 if is_attack else 0)

    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    accuracy = accuracy_score(y_test, y_pred) * 100
    precision = precision_score(y_test, y_pred) * 100
    recall = recall_score(y_test, y_pred) * 100
    f1 = f1_score(y_test, y_pred) * 100
    specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0

    return {
        'threshold': threshold,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'specificity': specificity,
        'tp': tp,
        'fp': fp,
        'tn': tn,
        'fn': fn
    }


def main():
    print("="*80)
    print("IMPROVED SVM MODEL PERFORMANCE TEST")
    print("="*80)
    print()
    print("Target Performance:")
    print("  üéØ False Positives: <500")
    print("  üéØ Accuracy: ‚â•85%")
    print("  üéØ Recall: ‚â•85%")
    print("  üéØ Precision: ‚â•82%")
    print()

    # Load test model
    model_file = 'improved_svm_model.pkl'

    if not os.path.exists(model_file):
        print(f"‚ùå Error: {model_file} not found")
        print("Please run train_improved_svm.py first")
        return 1

    print(f"Loading model: {model_file}")
    detector = ImprovedSVMAnomalyDetector.load_model(model_file)

    if not detector:
        print("‚ùå Failed to load model")
        return 1

    print("‚úÖ Model loaded successfully")
    print()

    # Load test data (70% train, 30% test split like in compare_models.py)
    csv_file = 'datasets/csic2010/CSIC_2010.csv'

    print("Loading CSIC 2010 dataset...")
    normal_samples = load_csic_samples(csv_file, 'Normal', max_samples=7000)
    attack_samples = load_csic_samples(csv_file, 'Anomalous', max_samples=4500)

    print(f"‚úÖ Loaded {len(normal_samples)} normal samples")
    print(f"‚úÖ Loaded {len(attack_samples)} attack samples")
    print()

    # Use same 70/30 split as training
    train_size_normal = int(len(normal_samples) * 0.7)
    train_size_attack = int(len(attack_samples) * 0.7)

    test_normal = normal_samples[train_size_normal:]
    test_attack = attack_samples[train_size_attack:]

    all_test = test_normal + test_attack
    y_test = [0] * len(test_normal) + [1] * len(test_attack)

    print(f"Test set: {len(test_normal)} normal + {len(test_attack)} attack = {len(all_test)} total")
    print()

    # Test multiple thresholds
    print("="*80)
    print("THRESHOLD OPTIMIZATION")
    print("="*80)
    print()

    thresholds = [0.50, 0.55, 0.60, 0.65, 0.70]
    results = []

    print(f"{'Thresh':<8} {'Acc':<8} {'Prec':<8} {'Rec':<8} {'F1':<8} {'Spec':<8} {'FP':<8} {'Status':<15}")
    print("-"*80)

    best_result = None
    best_score = 0

    for threshold in thresholds:
        result = evaluate_threshold(detector, all_test, y_test, threshold)
        results.append(result)

        # Check if meets all targets
        meets_fp = result['fp'] < 500
        meets_acc = result['accuracy'] >= 85
        meets_rec = result['recall'] >= 85

        status = ""
        if meets_fp and meets_acc and meets_rec:
            status = "‚úÖ ALL TARGETS"
        elif meets_fp:
            status = "‚úÖ FP TARGET"

        # Calculate composite score (prioritize FP < 500)
        if meets_fp:
            composite = result['accuracy'] + result['recall'] + result['precision']
            if composite > best_score:
                best_score = composite
                best_result = result

        print(f"{result['threshold']:<8.2f} "
              f"{result['accuracy']:>6.2f}% "
              f"{result['precision']:>6.2f}% "
              f"{result['recall']:>6.2f}% "
              f"{result['f1']:>6.2f}% "
              f"{result['specificity']:>6.2f}% "
              f"{result['fp']:>6d}  {status}")

    print("="*80)
    print()

    # Summary
    print("="*80)
    print("RESULTS SUMMARY")
    print("="*80)
    print()

    if best_result:
        print(f"üèÜ OPTIMAL THRESHOLD: {best_result['threshold']:.2f}")
        print()
        print("Performance at Optimal Threshold:")
        print(f"  {'Accuracy:':<20} {best_result['accuracy']:6.2f}%  {'‚úÖ' if best_result['accuracy'] >= 85 else '‚ö†Ô∏è'}")
        print(f"  {'Precision:':<20} {best_result['precision']:6.2f}%  {'‚úÖ' if best_result['precision'] >= 82 else '‚ö†Ô∏è'}")
        print(f"  {'Recall:':<20} {best_result['recall']:6.2f}%  {'‚úÖ' if best_result['recall'] >= 85 else '‚ö†Ô∏è'}")
        print(f"  {'F1-Score:':<20} {best_result['f1']:6.2f}%")
        print(f"  {'Specificity:':<20} {best_result['specificity']:6.2f}%")
        print()
        print("Confusion Matrix:")
        print(f"  True Positives:     {best_result['tp']:5d}")
        print(f"  False Positives:    {best_result['fp']:5d}  {'‚úÖ <500' if best_result['fp'] < 500 else '‚ùå ‚â•500'}")
        print(f"  True Negatives:     {best_result['tn']:5d}")
        print(f"  False Negatives:    {best_result['fn']:5d}")
        print()

        # Overall assessment
        if best_result['fp'] < 500 and best_result['accuracy'] >= 85 and best_result['recall'] >= 85:
            print("‚úÖ SUCCESS: All targets achieved!")
            print()
            print("Recommendation:")
            print(f"  ‚Ä¢ Use threshold {best_result['threshold']:.2f} (or {int(best_result['threshold'] * 100)} in UI)")
            print(f"  ‚Ä¢ Expected FP: ~{best_result['fp']} (well below 500 target)")
            print(f"  ‚Ä¢ Expected accuracy: ~{best_result['accuracy']:.1f}%")
            print(f"  ‚Ä¢ Expected recall: ~{best_result['recall']:.1f}%")
        else:
            print("‚ö†Ô∏è  PARTIAL SUCCESS: Some targets not met")
            print()
            if best_result['fp'] >= 500:
                print(f"  ‚ö†Ô∏è  FP still high: {best_result['fp']} (target: <500)")
            if best_result['accuracy'] < 85:
                print(f"  ‚ö†Ô∏è  Accuracy below target: {best_result['accuracy']:.2f}% (target: ‚â•85%)")
            if best_result['recall'] < 85:
                print(f"  ‚ö†Ô∏è  Recall below target: {best_result['recall']:.2f}% (target: ‚â•85%)")
    else:
        print("‚ö†Ô∏è  No threshold achieved FP <500 target")
        print()
        print("Consider:")
        print("  ‚Ä¢ Further increasing regularization (C=0.3)")
        print("  ‚Ä¢ Adding more normal samples to training")
        print("  ‚Ä¢ Ensemble with conservative model")

    print()
    print("="*80)

    return 0


if __name__ == '__main__':
    try:
        exit_code = main()
        import sys
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test interrupted by user")
        import sys
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        import sys
        sys.exit(1)
